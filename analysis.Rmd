---
title: 'STAT 447C: Final Project'
author: "Andrew Tran"
date: "`r Sys.Date()`"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE ,warning = FALSE)
library(tidyverse)
library(loo)
library(reshape2)
library(ggplot2)
library(pROC)
library(broom)
library(regclass)
library(rstan)
library(bayesplot)
```

## GitHub

https://github.com/nartyuh/ubc-stat-447C-project

## Introduction

Breast cancer is one the most common and malignant types of cancers. It is life-threatenting to many people around the world, especially women. There are many endeavours that motivate the research to find solutions for breast cancer treatment through the use of computational methods. The Breast Cancer Wisconsin is one of them. The challenge of this dataset is to develop inferential and predictive analyses on 569 images of tumor cell nucleus obtained by Fine-Needle Aspiration. This can help with imaging analysis of cancerous cells and provide a tool for early detection of cancer risk.

## Purpose of the project

The project conducts a comparison between the Frequentist and Bayesian approaches to logistic regression on the Breast Cancer Wisconsin dataset. We first look at the perspectives of both approaches in terms of variable selection when there is multicollinearity among the explanatory variables. Then we examine how the Frequentists and the Bayesians approach model comparison. Finally, we will perform model diagnostics using each approach's respective methodologies.

By making the comparison between Frequentist and Bayesian approaches, this project hopes to create a discourse around utilizing Bayesian approach for Breast Cancer Wisconsin.

## Data description

```{r, echo=FALSE}
df <- read.csv("data/breast_cancer_wisconsin.csv")
df <- as_tibble(df)
```

Our response variable is `Diagnosis`, which is categorical. The original dataset labels it as **M** (malignant) or **B** (benign). We want to transform it into binary values so that **1** and **0** are equivalent to **M** and **B** respectively.

```{r, echo=FALSE}
df$Diagnosis <- factor(df$Diagnosis)
df$Diagnosis <- as.numeric(df$Diagnosis) - 1
```

The Breast Cancer Wisconsin (Diagnostic) data set has 30 explanatory variables. However, in fact, they have 10 main features. For each feature, the mean, standard error, and worst/largest values are recorded. For example, for a single observation in the data, `radius1` is the mean of distances from center to points on the perimeter, `radius2` is the standard error those distances, and `radius3` is largest distance measured. For the sake of length, we are only interested in the means.

```{r, echo=FALSE}
df <- df[, 1:11]
```

These ten real-valued features are measurements taken and computed for each tumor cell nucleus:

\begin{center}
  \begin{tabular}{|c c c|} 
    \hline
    Variable & Type & Description \\
    \hline\hline
    Radius & Quantitative & Distance from center to points on the perimeter of the tumor \\ 
    \hline
    Texture & Quantitative & Gray-scale value \\
    \hline
    Perimeter & Quantitative & Size of the tumor\\
    \hline
    Area & Quantitative & Area of the tumor \\
    \hline
    Smoothness & Quantitative & Local variation in radius length \\
    \hline
    Compactness & Quantitative & $\frac{\text{perimeter}^2}{\text{area}} - 1$ \\
    \hline
    Concavity & Quantitative & Severity of concave portions of the contour\\
    \hline
    Concave points & Quantitative & Number of concave portions of the contour\\
    \hline
    Symmetry & Quantitative & Symmetrical measurement of the tumor\\
    \hline
    Fractal dimension & Quantitative & $\text{coastline approximation} - 1$\\
    \hline
  \end{tabular}
\end{center}

## Base model

### Frequentist

Fitting a base model with every explanatory variables included is a straightforward process with the Frequentist approach.

```{r}
frequentist.base_reg <- 
  glm(Diagnosis ~ ., data = df, family = binomial(link = "logit"))
```

### Bayesian

In contrast to the simplicity of the Frequentist approach, fitting a Bayesian regression model is a more hands-on process.

Let $\beta_0$ be the intercept parameter and $\beta_1,...\beta_{10}$ be the slope parameters for `radius`, `texture`, `perimeter`, `area`, `smoothness`, `compactness`, `concavity`, `concave_points`, `symmetry`, and `fractal_dimension` correspondingly.

To fit a Bayesian logistic regression to our data, we need to specify our Bayesian model. One of the important tasks when specifying our model is the selection of prior distributions. We often select generic weakly informative priors like $Normal(0,1)$ to perform regression task in Bayesian approach. This choice could work in our case as we are attempting to fit a logistic regression model. However, logistic regression can also become unstable from separation, a problem when the outcome variable separates a predictor variable perfectly. (Bayesian Data Analysis, p. 412) To avoid this problem, Gelman et al. (2008) suggested the Cauchy distribution with center 0 and scale set to 2.5 for the slopes and 10 for the intercept. We will employ this choice of prior to model Bayesian logistic regression. To proceed with this approach, it is required that we center and scale our non-binary variables to have mean 0 and standard deviation 0.5. (Gelman et al., 2008)

```{r, echo=FALSE}
bayesian.df <- df
bayesian.df$radius1 <- df$radius1 - mean(df$radius1)
bayesian.df$radius1 <- df$radius1/sd(df$radius1)/2

bayesian.df$texture1 <- df$texture1 - mean(df$texture1)
bayesian.df$texture1 <- df$texture1/sd(df$texture1)/2

bayesian.df$perimeter1 <- df$perimeter1 - mean(df$perimeter1)
bayesian.df$perimeter1 <- df$perimeter1/sd(df$perimeter1)/2

bayesian.df$area1 <- df$area1 - mean(df$area1)
bayesian.df$area1 <- df$area1/sd(df$area1)/2

bayesian.df$smoothness1 <- df$smoothness1 - mean(df$smoothness1)
bayesian.df$smoothness1 <- df$smoothness1/sd(df$smoothness1)/2

bayesian.df$compactness1 <- df$compactness1 - mean(df$compactness1)
bayesian.df$compactness1 <- df$compactness1/sd(df$compactness1)/2

bayesian.df$concavity1 <- df$concave_points1 - mean(df$concavity1)
bayesian.df$concavity1 <- df$concavity1/sd(df$concavity1)/2

bayesian.df$concave_points1 <- df$concave_points1 - mean(df$concave_points1)
bayesian.df$concave_points1 <- df$concave_points1/sd(df$concave_points1)/2

bayesian.df$symmetry1 <- df$symmetry1 - mean(df$symmetry1)
bayesian.df$symmetry1 <- df$symmetry1/sd(df$symmetry1)/2

bayesian.df$fractal_dimension1 <- df$fractal_dimension1 - mean(df$fractal_dimension1)
bayesian.df$fractal_dimension1 <- df$fractal_dimension1/sd(df$fractal_dimension1)/2
```

\begin{align}
  \beta_0 & \sim \text{Cauchy}(0, 10) \\
  \beta_i & \overset{\text{iid}}{\sim} \text{Cauchy}(0, 2.5) & \text{ for } i \in \{1,...,10\} \\
  y_n|\beta & \sim \text{Bern}(\text{logistic}(\beta_0 + \beta_1x_{n,1} + ... + \beta_{10}x_{n,10})) & n \in \{1,...,n_\text{obs}\}
\end{align}

```{r, echo=FALSE}
bayesian.base_reg.dat <- list(
  N = nrow(bayesian.df),
  x1 = bayesian.df$radius1,
  x2 = bayesian.df$texture1,
  x3 = bayesian.df$perimeter1,
  x4 = bayesian.df$area1,
  x5 = bayesian.df$smoothness1,
  x6 = bayesian.df$compactness1,
  x7 = bayesian.df$concavity1,
  x8 = bayesian.df$concave_points1,
  x9 = bayesian.df$symmetry1,
  x10 = bayesian.df$fractal_dimension1,
  y = bayesian.df$Diagnosis
)
```

```{r, results='hide'}
bayesian.base_model <- stan_model("base_logistic.stan")
bayesian.base_reg <- sampling(
  bayesian.base_model,
  data = bayesian.base_reg.dat,
  seed = 123, iter = 1000
)
```

## Model with the exclusion of highly correlated explanatory variables

By looking at the correlation matrix of explanatory variables (Appendix A.1), we identify the following highly correlated features: `radius`, `perimeter`, `area`, `compactness`, `concavity`, and `concave_points`.

<!-- \begin{center} -->
<!--   \begin{tabular}{|c c|}  -->
<!--     \hline -->
<!--     Pair & Correlation \\ -->
<!--     \hline\hline -->
<!--     radius and perimeter & 1 \\  -->
<!--     \hline -->
<!--     radius and area & 0.99 \\ -->
<!--     \hline -->
<!--     radius` and concave\_points & 0.82 \\ -->
<!--     \hline -->
<!--     perimeter and area & 0.99 \\ -->
<!--     \hline -->
<!--     perimeter and concave\_points & 0.85 \\ -->
<!--     \hline -->
<!--     area and concave\_points & 0.82 \\ -->
<!--     \hline -->
<!--     compactness and concavity & 0.88 \\ -->
<!--     \hline -->
<!--     compactness and concave\_points & 0.83 \\ -->
<!--     \hline -->
<!--     concavity and =concave\_points & 0.92 \\ -->
<!--     \hline -->
<!--   \end{tabular} -->
<!-- \end{center} -->

The above explanatory variables exhibit multicollinearity. In regression analysis, this is undesirable because it undermines the statistical significance of an independent variable, thus leading to skewed or misleading results that can negatively impact a statistical inference task. (Understanding Regression Analysis, p. 176) To resolve this problem, we need to investigate into the source of the high correlation values and remove the collinear variables when it is reasonable to do so.

- `perimeter` and `area` are highly correlated to `radius` and `concave_points`. This can be explained by the fact that both `radius` and `concave_points` determine the shape and outer structure of a cell nucleus so they will affect the calculation of `perimeter` and `area` to a certain proportional degree. Thus, we can exclude both `perimeter` and `area` from our list of features.
- `compactness` is highly correlated with `concavity` and `concave_points`. `compactness` is calculated using $\frac{\text{perimeter}^2}{\text{area}} - 1$. As we have discussed above, `concavity` and `concave_points` could affect the calculation of `perimeter` and `area`. As a result, it is also likely that `compactness` can be explained by `concavity` and `concave_points`. Thus, we can exclude `compactness` from our list of features.
- `concavity` and `concave_points` are highly correlated. `concavity` is the severity degree of concave portions, and `concave_points` is the number of concave points on a cell nucleus. Perhaps, they are proportional. Thus, we can exclude one of them from our list of features.

Note that we are only considering correlation values that are above 0.8. In practice, this choice is subjective. We do not want to remove too many predictors, otherwise we might get undesirable results. It is reasonable to sacrifice small amount of precision by including unimportant predictors for the general validity of our regression model. (Bayesian Data Analysis, p. 367)

### Frequentist

```{r}
frequentist.better_reg <- 
  glm(Diagnosis ~ radius1 + texture1 + smoothness1 + concavity1 + symmetry1 + 
        fractal_dimension1, data = df, family = binomial(link = "logit"))
```

### Bayesian

It is well-known that multicollinearity is undesirable in Frequentist approach to regression analysis. Why is it also undesirable in Bayesian approach? Multicollinearity could lead to high posterior variance of the regression coefficients, thus increasing uncertainty. Additionally, it makes the inference task highly sensitive to the model's assumption that $E(y|x, \theta)$ is linear in x. (Bayesian Data Analysis, p. 366) Thus, Bayesian approach could also benefit from the removal of multicollinear explanatory variables.

\begin{align}
  \beta_0 & \sim \text{Cauchy}(0, 10) \\
  \beta_i & \overset{\text{iid}}{\sim} \text{Cauchy}(0, 2.5) & i \in \{1,2,5,7,9,10\} \\
  y_n|\beta & \sim \text{Bern}(\text{logistic}(\beta_0 + \beta_1x_{n,1} + \beta_2x_{n,2} + \beta_5x_{n,5} + \beta_7x_{n,7} + \beta_9x_{n,9} + \beta_{10}x_{n,10})) & n \in \{1,...,n_\text{obs}\}
\end{align}

```{r, echo=FALSE}
bayesian.better_reg.dat <- list(
  N = nrow(bayesian.df),
  x1 = bayesian.df$radius1,
  x2 = bayesian.df$texture1,
  x5 = bayesian.df$smoothness1,
  x7 = bayesian.df$concavity1,
  x9 = bayesian.df$symmetry1,
  x10 = bayesian.df$fractal_dimension1,
  y = bayesian.df$Diagnosis
)
```

```{r, results='hide'}
bayesian.better_model <- stan_model("better_logistic.stan")
bayesian.better_reg <- sampling(
  bayesian.better_model,
  data = bayesian.better_reg.dat,
  seed = 123, iter = 1000
)
```

## Model comparison

### Frequentist

In the Frequentist approach, we often use Akaike information criterion (AIC) to compare different models on the same dataset. Models with lower AIC are better.

```{r}
AIC(frequentist.base_reg)
```

```{r}
AIC(frequentist.better_reg)
```

We can see that the AIC for our base model is lower than the one for our model with variable exclusion in the Frequentist approach. Even though the difference is not huge, it provides us insight into our analysis. The lower AIC for base model could imply that we might have removed too many predictors or important ones. With this information, we can adjust our variable selection.

### Bayesian

#### WAIC
\
AIC works really well in the Frequentist approach. However, it is not a preferable method of model comparison in Bayesian approach. Gelman et al. (2013) suggested that we use Watanabe-Akaike information criteria (WAIC) when comparing Bayesian models for the following reasons:

- WAIC can utilize the full posterior distribution whereas AIC cannot because AIC conditions on a point estimate
- WAIC works well with complex models (e.g. hierarchical) where the number of parameters increases with sample size
- WAIC corrects the effective number of parameters to adjust for overfitting

(Gelman et al., 2013)

In a similar fashion as AIC, models with lower WAIC are better.

```{r}
waic(extract_log_lik(bayesian.base_reg))$waic
```

```{r}
waic(extract_log_lik(bayesian.better_reg))$waic
```

Similar to the result of AIC for the Frequentist models, the base model has slightly lower WAIC than the model with variable exclusion. The implication is also similar to the Frequentist approach.

#### PSIS-LOO
\
Gelman et al. (2016) also suggested Pareto-smoothed importance sampling LOO (PSIS-LOO), or LOOIC, over WAIC for the following reasons:

- PSIS-LOO is more robust in cases with weak priors and influential observations
- PSIS-LOO and WAIC are asymptotically equivalent but WAIC may behave differently for small finite samples

(Gelman et al., 2016)

Again, in similar fashion to AIC and WAIC, models with lower LOOIC are better.

```{r}
loo(extract_log_lik(bayesian.base_reg))$looic
```

```{r}
loo(extract_log_lik(bayesian.better_reg))$looic
```

## Model diagnostics

### Frequentist

To diagnose a Frequentist logistic regression model, we can check for linearity, outlier, and multicollinearithy assumption. Below we will demonstrate 2 of the 3 diagnostic assumptions.

#### Outlier assumption
\
Frequentist logistic regression assumes that there are no outliers in the data. To test for this, we check if there are any data points that have a standardized residual larger than 3.

```{r}
model.data <- augment(frequentist.better_reg) %>% mutate(index = 1:n()) 
nrow(model.data %>% filter(abs(.std.resid) > 3))
```

As we can see, there is one outlier in our data. This violates the outlier assumption. Thus, we need to remove this observation and fit the regression model again.

#### Multicollinearity/VIF
\
Frequentist logistic regression assumes that there is no multicolinearity in our data. To test for this, we check for any VIF values larger than 5. Looking at the output in Appendix A.2, we can see there is no VIF value larger than 5. The multicollinearity assumption is satisfied. This makes sense because we have eliminated some variables with high correlation previously.

### Bayesian

To diagnose a Bayesian regression model, we can perform calibration analysis via cross-validation, prior predictive checks, MCMC diagnostics, etc. In general, there are more options in the Bayesian approach. Below we will demonstrate 2 diagnoses of the Bayesian workflow.

#### Prior predictive checks
\
Prior predictive checks can help assess the fit of a Bayesian model by evaluating potential replications involving new parameters. Looking at figure in Appendix A.3, we can see that the distribution of the average outcome, $\bar{y}$, stays about the same as we increase the number of predictors. Whether this is desirable or not is up to the objective of the modeler. 

#### MCMC diagnostics
\
MCMC diagnostics can help test for the speed of convergence. The MCMC trace plot in Appendix A.4 does not show any major deviance between chains. The MCMC rank histogram in Appendix A.5 shows that all histograms are approximately uniform. Thus, we can see that this is a case of fast mixing.

## Discussion

The goals of the Frequentist and Bayesian approaches in our case are technically the same: fit a logistic regression model to the Breast Cancer Wisconsin dataset for inference and prediction. However, each approach has a different way of performing the regression fit, thus having different output. The Frequentists assume that the population is fixed with an unknown quantity/parameter. Thus, the Frequentists give you point estimates. The Bayesians treat the unknown probabilistiically and think the world can always change. Thus, the Bayesians give you distributions. Because of this difference, it is difficult to directly compare the Frequentist and Bayesian regression models. This is likely the biggest limitation in this project.

\newpage

## Reference

Allen, M. P. (1997). Understanding regression analysis. Plenum Press.

Gelman, A., Carlin, J. B., Stern, H. S., & Dunson, D. B. (2013). Bayesian data analysis. Chapman and Hall/CRC.

Gelman, A., Jakulin, A., Pittau, M. G., & Su, Y.-S. (2008). A Weakly Informative Default Prior Distribution for Logistic and Other Regression Models. The Annals of Applied Statistics, 2(4), 1360–1383. http://www.jstor.org/stable/30245139

Gelman, A., Hwang, J. & Vehtari, A. Understanding predictive information criteria for Bayesian models. Stat Comput 24, 997–1016 (2013). https://doi.org/10.1007/s11222-013-9416-2

Vehtari, A., Gelman, A. & Gabry, J. Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27, 1413–1432 (2017). https://doi.org/10.1007/s11222-016-9696-4

## A. Appendix

### A.1 Correlation matrix of explanatory variables

```{r}
cor_mat <- round(cor(df[, 2:11]), 2)
melted_cor_mat <- melt(cor_mat)
ggplot(data = melted_cor_mat, aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1)) +
  xlab("") + ylab("") + ggtitle("Correlation Matrix")
```

### A.2 VIF for Frequentist model with variable exclusion

```{r}
VIF(frequentist.better_reg)
```

### A.3 Prior predictive checks

```{r}
suppressPackageStartupMessages(library(extraDistr))
```


```{r}
logistic_regression <- function(X) {
  b <- append(rcauchy(1, 0, 10), rcauchy(ncol(X) - 1, 0, 2.5))
  p <- plogis(as.vector(X %*% b))
  y <- rbern(nrow(X), p)
  mean(y)
}
```

```{r}
pred_prob = 0.9
n_obs = 100
n_sim = 100000
opar = par(mfrow=c(1,3))
for(n_pred in c(2,4,15)) {
    X = matrix(rbern(n_obs*n_pred, pred_prob), nrow=n_obs)
    simulated_ybars = replicate(n_sim, logistic_regression(X))
    hist(simulated_ybars, breaks=20, ylim=c(0,17000),
         main=paste(n_pred, "predictors"), xlab = "Average outcome")
}
```

### A.4 MCMC trace plot

```{r}
mcmc_trace(bayesian.better_reg, pars = c("b0")) + theme_minimal()
```

### A.5 MCMC rank histogram

```{r}
mcmc_rank_hist(bayesian.better_reg, pars = c("b0")) + theme_minimal()
```

### A.6 base_logistic.stan

```
data {
  // train data
  int<lower=0> N;
  vector[N] x1;
  vector[N] x2;
  vector[N] x3;
  vector[N] x4;
  vector[N] x5;
  vector[N] x6;
  vector[N] x7;
  vector[N] x8;
  vector[N] x9;
  vector[N] x10;
  int<lower=0,upper=1> y[N];
}

parameters {
  real b0;
  real b1;
  real b2;
  real b3;
  real b4;
  real b5;
  real b6;
  real b7;
  real b8;
  real b9;
  real b10;
}

model {
  b0 ~ cauchy(0, 10);
  b1 ~ cauchy(0, 2.5);
  b2 ~ cauchy(0, 2.5);
  b3 ~ cauchy(0, 2.5);
  b4 ~ cauchy(0, 2.5);
  b5 ~ cauchy(0, 2.5);
  b6 ~ cauchy(0, 2.5);
  b7 ~ cauchy(0, 2.5);
  b8 ~ cauchy(0, 2.5);
  b9 ~ cauchy(0, 2.5);
  b10 ~ cauchy(0, 2.5);
  y ~ bernoulli_logit(b0 + b1*x1 + b2*x2 + b3*x3 + b4*x4 + b5*x5 + b6*x6 + b7*x7 + b8*x8 + b9*x9 + b10*x10);
}

generated quantities {
  vector[N] log_lik;
  for (i in 1:N) {
    log_lik[i] = bernoulli_logit_lpmf(y[i] | b0 + b1*x1[i] + b2*x2[i] + b3*x3[i] + b4*x4[i] + b5*x5[i] + b6*x6[i] + b7*x7[i] + b8*x8[i] + b9*x9[i] + b10*x10[i]);
  }
}
```

### A.7 better_logistic.stan

```
data {
  // train data
  int<lower=0> N;
  vector[N] x1;
  vector[N] x2;
  vector[N] x5;
  vector[N] x7;
  vector[N] x9;
  vector[N] x10;
  int<lower=0,upper=1> y[N];
}

parameters {
  real b0;
  real b1;
  real b2;
  real b5;
  real b7;
  real b9;
  real b10;
}

model {
  b0 ~ cauchy(0, 10);
  b1 ~ cauchy(0, 2.5);
  b2 ~ cauchy(0, 2.5);
  b5 ~ cauchy(0, 2.5);
  b7 ~ cauchy(0, 2.5);
  b9 ~ cauchy(0, 2.5);
  b10 ~ cauchy(0, 2.5);
  y ~ bernoulli_logit(b0 + b1*x1 + b2*x2 + b5*x5 + b7*x7 + b9*x9 + b10*x10);
}

generated quantities {
  vector[N] log_lik;
  for (i in 1:N) {
    log_lik[i] = bernoulli_logit_lpmf(y[i] | b0 + b1*x1[i] + b2*x2[i] + b5*x5[i] + b7*x7[i] + b9*x9[i] + b10*x10[i]);
  }
}
```